{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium_stealth import stealth\n",
    "import time\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "page_num = 1\n",
    "prices = []\n",
    "room_size = []\n",
    "price_per_sqm = []\n",
    "condo_details = []\n",
    "station_element = []\n",
    "condo_names = []\n",
    "addresses = []\n",
    "agency = []\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "try:\n",
    "\n",
    "  while True:\n",
    "    url = f'https://www.ddproperty.com/en/property-for-rent/{page_num}?freetext=Bangkok&property_type=N&property_type_code%5B0%5D=CONDO&region_code=TH10&search=true'\n",
    "    print(f'page: {page_num}')\n",
    "        \n",
    "    try:\n",
    "      driver = webdriver.Chrome(options=options)\n",
    "      stealth(driver,\n",
    "              languages=[\"en-US\", \"en\"],\n",
    "              vendor=\"Google Inc.\",\n",
    "              platform=\"Win32\",\n",
    "              webgl_vendor=\"Intel Inc.\",\n",
    "              renderer=\"Intel Iris OpenGL Engine\",\n",
    "              fix_hairline=True,\n",
    "              )\n",
    "\n",
    "      driver.get(url)\n",
    "\n",
    "      soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "      # Price\n",
    "      prices.extend([price.text.strip() for price in soup.findAll('li', class_='list-price pull-left')])\n",
    "\n",
    "      # room type, size, how much per sqm\n",
    "      room_size.extend([ul.find('li', {'class': 'listing-floorarea pull-left'}).text.strip() if ul.find('li', {'class': 'listing-floorarea pull-left'}) else \"\"\n",
    "                        for ul in soup.find_all('ul', {'class': 'listing-features pull-left', 'data-automation-id': 'listing-card-other-details-txt'})])\n",
    "      \n",
    "      price_per_sqm.extend([ul.find('li', {'class': 'listing-floorarea pull-left'}).find_next('li').text.strip() if ul.find('li', {'class': 'listing-floorarea pull-left'}) else \"\"\n",
    "                            for ul in soup.find_all('ul', {'class': 'listing-features pull-left', 'data-automation-id': 'listing-card-other-details-txt'})])\n",
    "\n",
    "      # Furnished?, Built\n",
    "      condo_details.extend([detail.text.strip() for detail in soup.findAll('ul', class_='listing-property-type')])\n",
    "\n",
    "      # train station, how far from station.\n",
    "      station_element.extend([i for i in soup.findAll('div', class_='row')])\n",
    "      station_extract = [sta.find('i', class_='pgicon pgicon-walk') for sta in station_element\n",
    "                        if 'col-xs-12 col-sm-12 listing-description' in str(sta) or 'col-xs-12 col-sm-7 listing-description' in str(sta)]\n",
    "      stations = [name.parent.text.strip() if 'pgicon pgicon-walk' in str(name) else \"\" for name in station_extract]\n",
    "\n",
    "      # Amount of bedroom and bathroom\n",
    "      room_amount = [r.find('li', class_='listing-rooms pull-left') if 'listing-rooms pull-left' in str(r) else r.find('h3', class_='h4') if r.find('h3', class_='h4') else \"\" for r in station_element]\n",
    "      bathroom = [ba.find('span', class_='bath').text if ba and ba.find('span', class_='bath') else \"\" for ba in room_amount if ba]\n",
    "      bedroom = [be.find('span', class_='bed').text if be and be.find('span', class_='bed') else\n",
    "                be.find('span', class_='studio').text if be and be.find('span', class_='studio') else \"\"  \n",
    "                for be in room_amount if be]\n",
    "\n",
    "      # Condo name\n",
    "      condo_names.extend([name.find('h3', class_='h4').text.strip() for name in soup.findAll('div', class_='header-container')])\n",
    "\n",
    "      # Location\n",
    "      addresses.extend([address.text.strip() for address in soup.findAll('p', class_=\"listing-location ellipsis\")])\n",
    "\n",
    "      agency.extend([agen.find('span', class_='name').text for agen in soup.findAll('div', class_='featured-description col-xs-12 col-sm-7')])\n",
    "\n",
    "      link = [link.find('a', class_='nav-link').get('href') for link in station_element\n",
    "                        if 'col-xs-12 col-sm-12 listing-description' in str(link) or 'col-xs-12 col-sm-7 listing-description' in str(link)]\n",
    "\n",
    "      if soup.find('li', class_=\"pagination-next disabled\"):\n",
    "        print(\"this is the last page\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Error on page {page_num}: {e}\")\n",
    "      break\n",
    "\n",
    "    page_num += 1\n",
    "\n",
    "    # scrap data in specific page   \n",
    "    # if page_num > 100:\n",
    "    #   break\n",
    "\n",
    "finally:\n",
    "  driver.quit()\n",
    "\n",
    "filtered_con_names = [name.replace('For Rent - ', '').replace(', Bangkok', '').strip() for name in condo_names if name and name.startswith('For Rent - ')]\n",
    "\n",
    "stripped_addresses = [address.lstrip('- ').strip() for address in addresses]\n",
    "\n",
    "df = pd.DataFrame({'name':condo_names, 'price_p_month': prices, 'address': stripped_addresses, 'station': stations, 'condo_details': condo_details,\n",
    "                  'room_size': room_size, 'bedroom': bedroom, 'bathroom': bathroom, 'price_per_sqm': price_per_sqm, 'agency': agency, 'link': link})\n",
    "\n",
    "df.to_csv(r'C:\\to\\my\\path\\condo_bkk_2024_2_300.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# export as SQLite\n",
    "# conn = sqlite3.connect('bkk_condo.db')\n",
    "# df.to_sql('condo_bkk_2024', con=conn, if_exists='append')\n",
    "# conn.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
